{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardobocci/ml-stock-market/blob/main/0.master_thesis_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUlXOw6xg3Ip"
      },
      "source": [
        "Pipeline Steps:\n",
        "1. Load data for all ETFs\n",
        "2. Perform column calculations (eg. returns and indicators)\n",
        "3. Handle Outliers\n",
        "3. Transform data (eg. dates to cyclical coordinates)\n",
        "4. Enforce Stationarity\n",
        "5. Save Stationary Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries and Data Loading"
      ],
      "metadata": {
        "id": "eZ7G6CjJUtyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install feature_engine\n",
        "!pip install statsmodels --upgrade"
      ],
      "metadata": {
        "id": "vlgOYym69hfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wzGuodDcPrg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz \n",
        "!tar xvzf ta-lib-0.4.0-src.tar.gz\n",
        "import os\n",
        "os.chdir('ta-lib') # Can't use !cd in colab\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install\n",
        "os.chdir('../')\n",
        "!pip install TA-Lib\n",
        "!pip install -U git+https://github.com/twopirllc/pandas-ta\n",
        "import talib, pandas_ta as ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXPESMBBArpO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import TimeSeriesSplit \n",
        "from feature_engine.creation import CyclicalFeatures\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "workbook = gc.open('all_etfs_OHLCV')\n",
        "sheet_titles = []\n",
        "for sheet in workbook.worksheets():\n",
        "  sheet_titles.append(sheet.title)\n",
        "\n",
        "dict_of_sheets = {}\n",
        "for sheet_title in sheet_titles:\n",
        "  sheet = workbook.worksheet(sheet_title)\n",
        "  values = sheet.get_all_values()\n",
        "  dict_of_sheets[sheet_title] = values\n",
        "\n",
        "keys = list(dict_of_sheets)\n",
        "etfs = {}\n",
        "for etf in keys:\n",
        "  etfs[etf] = pd.DataFrame.from_records(dict_of_sheets[etf])\n",
        "  cols = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
        "  etfs[etf].columns = cols\n",
        "  etfs[etf] = etfs[etf][1:]\n",
        "  etfs[etf]['date'] = pd.to_datetime(etfs[etf]['date'], format=\"%d/%m/%Y\")\n",
        "  etfs[etf].set_index('date', inplace=True)\n",
        "  etfs[etf] = etfs[etf].apply(pd.to_numeric, errors='coerce')\n",
        "  etfs[etf] = etfs[etf].fillna(method=\"ffill\")\n",
        "  etfs[etf].dropna(axis = 0, inplace = True)    "
      ],
      "metadata": {
        "id": "sRjjJ4zS9IfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km7R7tNksVA8"
      },
      "source": [
        "# Indicators"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_ta_indicators(original_df):\n",
        "  df = original_df.copy()\n",
        "  #SMA\n",
        "  df['sma_50'] = df.ta.sma(close=df.close, length=50)\n",
        "  df['sma_200'] = df.ta.sma(close=df.close, length=200)\n",
        "  df.loc[(df.sma_50 > df.sma_200) & (df.sma_50.shift(1) <= df.sma_200.shift(1)), 'sma_cross'] = 1\n",
        "  df.loc[(df.sma_50 < df.sma_200) & (df.sma_50.shift(1) >= df.sma_200.shift(1)), 'sma_cross'] = -1\n",
        "  df.sma_cross.fillna(0, inplace=True)\n",
        "\n",
        "  #WMA\n",
        "  df['wma_50'] = df.ta.wma(close=df.close, length=50)\n",
        "  df['wma_200'] = df.ta.wma(close=df.close, length=200)\n",
        "  df.loc[(df.wma_50 > df.wma_200) & (df.wma_50.shift(1) <= df.wma_200.shift(1)), 'wma_cross'] = 1\n",
        "  df.loc[(df.wma_50 < df.wma_200) & (df.wma_50.shift(1) >= df.wma_200.shift(1)), 'wma_cross'] = -1\n",
        "  df.wma_cross.fillna(0, inplace=True)\n",
        "\n",
        "  #EMA\n",
        "  df['ema_12'] = df.ta.ema(close=df.close, length=12)\n",
        "  df['ema_26'] = df.ta.ema(close=df.close, length=26)\n",
        "  df.loc[(df.ema_12 > df.ema_26) & (df.ema_12.shift(1) <= df.ema_26.shift(1)), 'ema_cross'] = 1\n",
        "  df.loc[(df.ema_12 < df.ema_26) & (df.ema_12.shift(1) >= df.ema_26.shift(1)), 'ema_cross'] = -1\n",
        "  df.ema_cross.fillna(0, inplace=True)\n",
        "\n",
        "  #DEMA\n",
        "  df['dema_50'] = df.ta.dema(close=df.close, length=50)\n",
        "  df['dema_200'] = df.ta.dema(close=df.close, length=200)\n",
        "  df.loc[(df.dema_50 > df.dema_200) & (df.dema_50.shift(1) <= df.dema_200.shift(1)), 'dema_cross'] = 1\n",
        "  df.loc[(df.dema_50 < df.dema_200) & (df.dema_50.shift(1) >= df.dema_200.shift(1)), 'dema_cross'] = -1\n",
        "  df.dema_cross.fillna(0, inplace=True)\n",
        "\n",
        "  #TRIMA\n",
        "  df['trima_50'] = df.ta.trima(close=df.close, length=50)\n",
        "  df['trima_200'] = df.ta.trima(close=df.close, length=200)\n",
        "  df.loc[(df.trima_50 > df.trima_200) & (df.trima_50.shift(1) <= df.trima_200.shift(1)), 'trima_cross'] = 1\n",
        "  df.loc[(df.trima_50 < df.trima_200) & (df.trima_50.shift(1) >= df.trima_200.shift(1)), 'trima_cross'] = -1\n",
        "  df.trima_cross.fillna(0, inplace=True)\n",
        "\n",
        "  #HULL\n",
        "  df['hull_50'] = df.ta.hma(close=df.close, length=50)\n",
        "  df['hull_200'] = df.ta.hma(close=df.close, length=200)\n",
        "  df['hull_12'] = df.ta.hma(close=df.close, length=12)\n",
        "  df['hull_26'] = df.ta.hma(close=df.close, length=26)\n",
        "  df.loc[(df.hull_50 > df.hull_200) & (df.hull_50.shift(1) <= df.hull_200.shift(1)), 'hull_slow_cross'] = 1\n",
        "  df.loc[(df.hull_50 < df.hull_200) & (df.hull_50.shift(1) >= df.hull_200.shift(1)), 'hull_slow_cross'] = -1\n",
        "  df.hull_slow_cross.fillna(0, inplace=True)\n",
        "  df.loc[(df.hull_12 > df.hull_26) & (df.hull_12.shift(1) <= df.hull_26.shift(1)), 'hull_fast_cross'] = 1\n",
        "  df.loc[(df.hull_12 < df.hull_26) & (df.hull_12.shift(1) >= df.hull_26.shift(1)), 'hull_fast_cross'] = -1\n",
        "  df.hull_fast_cross.fillna(0, inplace=True)\n",
        "\n",
        "  #STOCH\n",
        "  stoch = df.ta.stoch(close=df.close,high=df.high, low=df.low)\n",
        "  stoch.columns = ['k', 'd']\n",
        "  df['stoch_d'] = stoch.d\n",
        "  df['stoch_k'] = stoch.k\n",
        "\n",
        "  #RSI\n",
        "  df['rsi'] = df.ta.rsi(close=df.close)\n",
        "\n",
        "  #MACD\n",
        "  macd = df.ta.macd(close=df.close)\n",
        "  macd.columns = ['macd', 'histogram', 'signal']\n",
        "  df['macd_hist'] = macd.histogram\n",
        "  df.loc[(df.macd_hist > 0) & (df.macd_hist.shift(1) <= 0), 'macd_hist_cross'] = 1\n",
        "  df.loc[(df.macd_hist < 0) & (df.macd_hist.shift(1) >= 0), 'macd_hist_cross'] = -1\n",
        "  df.macd_hist_cross.fillna(0, inplace=True)\n",
        "\n",
        "  #WILLIAMS\n",
        "  df['williams_r'] = df.ta.willr(close=df.close,high=df.high, low=df.low)\n",
        "\n",
        "  #ADO\n",
        "  df['ado'] = df.ta.adosc(close=df.close,high=df.high, low=df.low, open=df.open, volume=df.volume)\n",
        "\n",
        "  #CCI\n",
        "  df['cci'] = df.ta.ad(close=df.close,high=df.high, low=df.low)\n",
        "\n",
        "  #ROC\n",
        "  df['roc'] = df.ta.roc(close=df.close, length=12)\n",
        "\n",
        "  #DI\n",
        "  #Here the disparity index may use any type of moving average, hull could be a better option than sma.\n",
        "  df['di'] = df.ta.sma(close=df.close, length=14)\n",
        "  df['di'] = (df.close - df.di) / df.di * 100\n",
        "\n",
        "  #PPO\n",
        "  ppo = df.ta.ppo(close=df.close)\n",
        "  ppo.columns = ['ppo', 'histogram', 'signal']\n",
        "  df['ppo_hist'] = ppo.histogram\n",
        "\n",
        "  #PVO\n",
        "  pvo = df.ta.pvo(volume=df.volume)\n",
        "  pvo.columns = ['pvo', 'histogram', 'signal']\n",
        "  df['pvo_hist'] = pvo.histogram\n",
        "\n",
        "  #PSY\n",
        "  df['psy'] = df.ta.psl(close=df.close, open=df.open)\n",
        "\n",
        "  #ADX\n",
        "  adx = df.ta.adx(high=df.high, low=df.low)\n",
        "  adx.columns = ['adx', 'dmp', 'dmm']\n",
        "  adx['dmi'] = abs(adx.dmp - adx.dmm) / abs(adx.dmp + adx.dmm) * 100\n",
        "  df['adx'] = adx.loc[:, 'adx']\n",
        "  df['diplus'] = adx.loc[:, 'dmp']\n",
        "  df['diminus'] = adx.loc[:, 'dmm']\n",
        "  df['dmi'] = adx.loc[:, 'dmi']\n",
        "\n",
        "  #OBV\n",
        "  df['obv'] = df.ta.obv(close=df.close, volume=df.volume)\n",
        "\n",
        "  #KLINGER\n",
        "  klinger = df.ta.kvo(close=df.close, volume=df.volume, high=df.high, low=df.low)\n",
        "  klinger.columns = ['klo', 'signal']\n",
        "  klinger['histogram'] = klinger.klo - klinger.signal\n",
        "  df['klo_hist'] = klinger.histogram\n",
        "\n",
        "  #MFI\n",
        "  df['mfi'] = df.ta.mfi(close=df.close, volume=df.volume, high=df.high, low=df.low)\n",
        "\n",
        "  #CMF\n",
        "  df['cmf'] = df.ta.cmf(close=df.close, volume=df.volume, high=df.high, low=df.low, open=df.open)\n",
        "\n",
        "  #ATR\n",
        "  df['atr'] = df.ta.atr(close=df.close, high=df.high, low=df.low)\n",
        "\n",
        "  #NATR\n",
        "  df['natr'] = df.ta.natr(close=df.close, high=df.high, low=df.low)\n",
        "\n",
        "  #BOLLINGER\n",
        "  bollinger = df.ta.bbands(close=df.close)\n",
        "  bollinger.columns = ['lower', 'mid', 'upper', 'bandwidth', 'percent']\n",
        "  df['bolu'] = bollinger.upper\n",
        "  df['bold'] = bollinger.lower\n",
        "  df['bol_width'] = bollinger.bandwidth\n",
        "  df['bol_pct'] = bollinger.percent\n",
        "\n",
        "  #KELTNER\n",
        "  keltner = df.ta.kc(close=df.close, high=df.high, low=df.low)\n",
        "  keltner.columns = ['lower', 'basis', 'upper']\n",
        "  df['kelu'] = keltner.upper\n",
        "  df['keld'] = keltner.lower\n",
        "\n",
        "  #ACCBANDS\n",
        "  accbands = df.ta.accbands(close=df.close, high=df.high, low=df.low)\n",
        "  accbands.columns = ['lower', 'middle', 'upper']\n",
        "  df['abau'] = accbands.upper\n",
        "  df['abal'] = accbands.lower\n",
        "\n",
        "  #ENVELOPE\n",
        "  envelope = df.ta.sma(close=df.close, length=20)\n",
        "  df['maeu'] = envelope * 1.05\n",
        "  df['maed'] = envelope * 0.95\n",
        "\n",
        "  #NVI PVI\n",
        "  df['nvi'] = df.ta.nvi(close=df.close, volume=df.volume)\n",
        "  df['pvi'] = df.ta.pvi(close=df.close, volume=df.volume)\n",
        "\n",
        "  #VWMA\n",
        "  df['vwma'] = df.ta.vwma(close=df.close, volume=df.volume)\n",
        "\n",
        "  #PSAR\n",
        "  psar = df.ta.psar(close=df.close, high=df.high, low=df.low)\n",
        "  psar.columns = ['long', 'short', 'acceleration', 'reversal']\n",
        "  df['long_psar'] = psar.long\n",
        "  df['short_psar'] = psar.short\n",
        "  df['psar_acc'] = psar.acceleration\n",
        "  df.long_psar.fillna(df.close, inplace=True)\n",
        "  df.short_psar.fillna(df.close, inplace=True)\n",
        "  df['psar_hist'] = (df.close - df.short_psar) + (df.close - df.long_psar)\n",
        "  df['psar_cross'] = 0\n",
        "  df.loc[(df.psar_hist > 0) & (df.psar_hist.shift(1) <= 0), 'psar_cross'] = 1\n",
        "  df.loc[(df.psar_hist < 0) & (df.psar_hist.shift(1) >= 0), 'psar_cross'] = -1\n",
        "  df.drop(['long_psar', 'short_psar'], axis=1, inplace=True)\n",
        "\n",
        "  #APO\n",
        "  df['apo'] = df.ta.apo(close=df.close)\n",
        "\n",
        "  #AROON\n",
        "  aroon = df.ta.aroon(close=df.close)\n",
        "  aroon.columns = ['positive', 'negative', 'oscillator']\n",
        "  df['arp'] = aroon.positive\n",
        "  df['arn'] = aroon.negative\n",
        "  df['aro'] = aroon.oscillator\n",
        "\n",
        "  #CMO\n",
        "  df['cmo'] = df.ta.cmo(close=df.close)\n",
        "\n",
        "  #CFO\n",
        "  df['cfo'] = df.ta.cfo(close=df.close)\n",
        "\n",
        "  #COCU\n",
        "  df['cocu'] = df.ta.coppock(close=df.close)\n",
        "\n",
        "  #DPO\n",
        "  df['dpo'] = df.ta.dpo(close=df.close, lookahead=False)\n",
        "\n",
        "  #EOM\n",
        "  df['eom'] = df.ta.eom(close=df.close, high=df.high, low=df.low, volume=df.volume)\n",
        "\n",
        "  #FI\n",
        "  df['fi'] = df.ta.efi(close=df.close, volume=df.volume)\n",
        "\n",
        "  #MI\n",
        "  df['mi'] = df.ta.massi(high=df.high, low=df.low)\n",
        "\n",
        "  #TP\n",
        "  df['tp'] = df.ta.hlc3(high=df.high, low=df.low, close=df.close)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "gcuN8mAnZkC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ta_etfs = {}\n",
        "for etf in keys:\n",
        "  ta_etfs[etf] = add_ta_indicators(original_df=etfs[etf])"
      ],
      "metadata": {
        "id": "HWBsmyafADKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkDHYYQD3J2B"
      },
      "source": [
        "For dummy columns, 1 = buy signal, -1 = sell signal and 0 = no signal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdiTeS4pL5mT"
      },
      "source": [
        "# Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL5V-TNhR-b-"
      },
      "source": [
        "Log returns is the dependent variable, day/month/year are extracted from the dates variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ-iXmjCcI0R"
      },
      "source": [
        "Date cyclical transformation:\n",
        "To retain the infromation that days and months are cyclical, they are transformed using sin-cos coordinates."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All columns need to be shifted by 1 to avoid data leakage. We need to use past information, not same day to predict returns."
      ],
      "metadata": {
        "id": "S65lxmVLHxBw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-43PbJYtRk83"
      },
      "outputs": [],
      "source": [
        "cyclical = CyclicalFeatures(variables=['day', 'month'])\n",
        "\n",
        "for etf in keys:\n",
        "  #Returns\n",
        "  ta_etfs[etf]['log_returns'] = np.log(ta_etfs[etf].close) - np.log(ta_etfs[etf].close.shift(1))\n",
        "\n",
        "  #Date Features\n",
        "  ta_etfs[etf]['day'] = ta_etfs[etf].index.day \n",
        "  ta_etfs[etf]['month'] = ta_etfs[etf].index.month \n",
        "  ta_etfs[etf]['year'] = ta_etfs[etf].index.year \n",
        "\n",
        "  #Date cyclical transformation\n",
        "  ta_etfs[etf] = cyclical.fit_transform(ta_etfs[etf])\n",
        "\n",
        "  #Shifting\n",
        "  mask = ~(ta_etfs[etf].columns.isin(['log_returns']))\n",
        "  cols_to_shift = ta_etfs[etf].columns[mask]\n",
        "  ta_etfs[etf][cols_to_shift] = ta_etfs[etf][cols_to_shift].shift(1)\n",
        "  ta_etfs[etf]['last_log_return'] = ta_etfs[etf].log_returns.shift(1)\n",
        "\n",
        "  #NA\n",
        "  ta_etfs[etf].dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier Handling"
      ],
      "metadata": {
        "id": "J4xPo2-5RTby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen method: Flagging (dummy variable) through IQR method.\n",
        "\n",
        "Preferred to other methods such as imputation or deletion to preserve information. Assumption: Outliers in the data occur because of external factors that influence the DGP, and not because of data quality errors."
      ],
      "metadata": {
        "id": "t3GvMaPqIIXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for etf in keys:\n",
        "  q3 = ta_etfs[etf].log_returns.quantile(0.75)\n",
        "  q1 = ta_etfs[etf].log_returns.quantile(0.25)\n",
        "  iqr = q3- q1\n",
        "\n",
        "  cutoff = 3\n",
        "  lower = q1 - cutoff * iqr\n",
        "  upper = q3 + cutoff * iqr\n",
        "\n",
        "  #-1 for low values, 0 for normal, 1 for high values\n",
        "  ta_etfs[etf].loc[ta_etfs[etf]['log_returns'] > upper, 'outlier'] = 1\n",
        "  ta_etfs[etf].loc[ta_etfs[etf]['log_returns'] < lower, 'outlier'] = -1\n",
        "  ta_etfs[etf].outlier.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "KGRyn27CRVZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop Nulls"
      ],
      "metadata": {
        "id": "_dxWUMAoN4Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for etf in keys:\n",
        "  ta_etfs[etf] = ta_etfs[etf].dropna()"
      ],
      "metadata": {
        "id": "hfxpcm9aN3qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Descriptive Statistics"
      ],
      "metadata": {
        "id": "DBAmMhO1T908"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_description = {}\n",
        "for etf in keys:\n",
        "  base_description[etf] = ta_etfs[etf][[\n",
        "      'log_returns', 'outlier', 'open', 'high', 'low', 'close', 'volume'\n",
        "  ]].describe()\n",
        "\n",
        "full_dataset_description = {}\n",
        "for etf in keys:\n",
        "  full_dataset_description[etf] = ta_etfs[etf].describe()"
      ],
      "metadata": {
        "id": "E0Lzt2MTUBDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Stationarity"
      ],
      "metadata": {
        "id": "Zz_pRZyanl-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_cols = ['sma_cross','wma_cross','ema_cross','dema_cross',\n",
        "                 'trima_cross','hull_slow_cross', 'hull_fast_cross',\n",
        "                 'macd_hist_cross','psar_cross', 'day', 'month','year',\n",
        "                 'day_sin', 'day_cos','month_sin', 'month_cos', 'outlier']\n",
        "columns = [x for x in ta_etfs[etf].columns if x not in excluded_cols]"
      ],
      "metadata": {
        "id": "VyONKJ5VRKtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "adf_list = []\n",
        "kpss_list = []\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import kpss\n",
        "def kpss_test(timeseries):\n",
        "    kpsstest = kpss(timeseries, regression=\"c\", nlags='auto')\n",
        "    kpss_output = pd.Series(\n",
        "        kpsstest[0:3], index=[\"Test Statistic\", \"p-value\", \"Lags Used\"]\n",
        "    )\n",
        "    for key, value in kpsstest[3].items():\n",
        "        kpss_output[\"Critical Value (%s)\" % key] = value\n",
        "    if kpss_output['p-value'] < 0.05:\n",
        "      kpss_list.append(False)\n",
        "    else:\n",
        "      kpss_list.append(True)\n",
        "\n",
        "def adf_test(timeseries):\n",
        "    dftest = adfuller(timeseries, autolag=\"AIC\")\n",
        "    dfoutput = pd.Series(\n",
        "        dftest[0:4],\n",
        "        index=[\n",
        "            \"Test Statistic\",\n",
        "            \"p-value\",\n",
        "            \"#Lags Used\",\n",
        "            \"Number of Observations Used\",\n",
        "        ],\n",
        "    )\n",
        "    for key, value in dftest[4].items():\n",
        "        dfoutput[\"Critical Value (%s)\" % key] = value\n",
        "    if dfoutput['p-value'] < 0.05:\n",
        "      adf_list.append(True)\n",
        "    else:\n",
        "      adf_list.append(False)\n",
        "\n",
        "stationarity={}\n",
        "for etf in keys:\n",
        "  for col in columns: \n",
        "    adf_test(ta_etfs[etf][col].dropna())\n",
        "    kpss_test(ta_etfs[etf][col].dropna())\n",
        "  stationarity[etf]= pd.DataFrame(\n",
        "    {'Feature':columns,\n",
        "     'Stationary_ADF':adf_list,\n",
        "     'Stationary_KPSS':kpss_list})\n",
        "  kpss_list = []\n",
        "  adf_list = []"
      ],
      "metadata": {
        "id": "w1ZwT02bnoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case 1: Both tests conclude that the series is not stationary - The series is not stationary;\n",
        "\n",
        "Case 2: Both tests conclude that the series is stationary - The series is stationary;\n",
        "\n",
        "Case 3: KPSS indicates stationarity and ADF indicates non-stationarity - The series is trend stationary. Trend needs to be removed to make series strict stationary. The detrended series is checked for stationarity;\n",
        "\n",
        "Case 4: KPSS indicates non-stationarity and ADF indicates stationarity - The series is difference stationary. Differencing is to be used to make series stationary. The differenced series is checked for stationarity."
      ],
      "metadata": {
        "id": "WHv-SH74hRAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "stationary_etfs = {}\n",
        "for etf in keys:\n",
        "  stationary_cols =  stationarity[etf].loc[(stationarity[etf].Stationary_ADF == True) & (stationarity[etf].Stationary_KPSS == True), 'Feature'].to_list()\n",
        "  non_stationary_cols =  stationarity[etf].loc[(stationarity[etf].Stationary_ADF == False) | (stationarity[etf].Stationary_KPSS == False), 'Feature'].to_list()\n",
        "\n",
        "  stationary = ta_etfs[etf].copy()\n",
        "  adf_list = []\n",
        "  kpss_list = []\n",
        "  persistent_non_stationary = []\n",
        "  excluded_cols = ['sma_cross','wma_cross','ema_cross','dema_cross',\n",
        "                  'trima_cross','hull_slow_cross', 'hull_fast_cross',\n",
        "                  'macd_hist_cross','psar_cross', 'day', 'month',\n",
        "                  'year', 'day_sin', 'day_cos','month_sin', 'month_cos']\n",
        "  columns = [x for x in stationary.columns if x not in excluded_cols]\n",
        "  for col in columns:\n",
        "    tested_series = []\n",
        "    exit=False\n",
        "    if col in non_stationary_cols:\n",
        "      new_col = 'd_' + col\n",
        "      tested_series = stationary[col].diff(periods=1) \n",
        "      adf_test(tested_series.dropna())\n",
        "      kpss_test(tested_series.dropna())\n",
        "      if (adf_list[-1]==True) & (kpss_list[-1]==True):\n",
        "        exit=True\n",
        "      if exit==False and ~(stationary[col].values<=0).any():\n",
        "        new_col = 'log_' + col\n",
        "        tested_series = np.log(stationary[col])\n",
        "        adf_test(tested_series.dropna())\n",
        "        kpss_test(tested_series.dropna())\n",
        "        if (adf_list[-1]==True) & (kpss_list[-1]==True):\n",
        "          exit=True\n",
        "      if exit==False:\n",
        "        new_col = 'exp_' + col\n",
        "        tested_series = np.exp(stationary[col]/10000000000)\n",
        "        adf_test(tested_series.dropna())\n",
        "        kpss_test(tested_series.dropna())\n",
        "        if (adf_list[-1]==True) & (kpss_list[-1]==True):\n",
        "          exit=True\n",
        "      if exit==False:\n",
        "        for i in range(2,4):\n",
        "          new_col = f'd_{i}_' + col\n",
        "          tested_series = stationary[col].diff(periods=i) \n",
        "          adf_test(tested_series.dropna())\n",
        "          kpss_test(tested_series.dropna())\n",
        "          if (adf_list[-1]==True) & (kpss_list[-1]==True):\n",
        "            exit=True\n",
        "            break\n",
        "      if exit==False and ~(stationary[col].values<=0).any():\n",
        "        new_col = 'log_d_' + col\n",
        "        tested_series = np.log(stationary[col]) - np.log(stationary[col].shift(1))\n",
        "        adf_test(tested_series.dropna())\n",
        "        kpss_test(tested_series.dropna())\n",
        "        if (adf_list[-1]==True) & (kpss_list[-1]==True):\n",
        "          exit=True\n",
        "      if exit==False:\n",
        "        new_col = 'exp_d_' + col\n",
        "        tested_series = np.exp(stationary[col]/10000000000) - np.exp((stationary[col]/10000000000).shift(1))\n",
        "        adf_test(tested_series.dropna())\n",
        "        kpss_test(tested_series.dropna())\n",
        "        if (adf_list[-1]==True) & (kpss_list[-1]==True):\n",
        "          exit=True\n",
        "      if exit==False:\n",
        "        for i in [7,30,365]:\n",
        "          new_col = f'd_{i}_' + col\n",
        "          tested_series = stationary[col].diff(periods=i) \n",
        "          adf_test(tested_series.dropna())\n",
        "          kpss_test(tested_series.dropna())\n",
        "          if (adf_list[-1]==True) & (kpss_list[-1]==True):\n",
        "            exit=True\n",
        "            break\n",
        "      if exit==True:\n",
        "        stationary[new_col] = tested_series\n",
        "      else:\n",
        "        persistent_non_stationary.append(col)\n",
        "  stationary.dropna(inplace=True)\n",
        "  cols_to_drop = [x for x in stationary.columns if x in non_stationary_cols]\n",
        "  stationary.drop(cols_to_drop, axis=1, inplace=True)\n",
        "  stationary_etfs[etf] = stationary"
      ],
      "metadata": {
        "id": "suiloyyaFS9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Persistent non-stationary vars are not used in the models."
      ],
      "metadata": {
        "id": "Fnxfj1z4NmlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Sheets to Gdrive"
      ],
      "metadata": {
        "id": "YBN-EN7caBsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for etf in keys:\n",
        "  path = f'/content/drive/My Drive/Bocci_Machine_Learning_Returns/Data/{etf}.csv'\n",
        "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "    stationary_etfs[etf].to_csv(f)\n",
        "\n",
        "  path = f'/content/drive/My Drive/Bocci_Machine_Learning_Returns/Data/full_describe_{etf}.csv'\n",
        "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "    full_dataset_description[etf].to_csv(f)\n",
        "\n",
        "  path = f'/content/drive/My Drive/Bocci_Machine_Learning_Returns/Data/base_describe_{etf}.csv'\n",
        "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "    base_description[etf].to_csv(f)"
      ],
      "metadata": {
        "id": "I9yNTpHpaIS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNM18KpgzcVUBj1Q4qlll8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}