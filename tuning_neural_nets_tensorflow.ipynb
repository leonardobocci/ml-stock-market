{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardobocci/ml-stock-market/blob/main/tuning_neural_nets_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY4ttPWBX9Wq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install featurewiz==0.1.996\n",
        "!pip install tscv\n",
        "!pip install neptune\n",
        "!pip install neptune-tensorflow-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkPlomdTX8Da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d5ea60d-8c93-41cf-f8f2-61ea77f49489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported 0.1.996 version. Select nrows to a small number when running on huge datasets.\n",
            "output = featurewiz(dataname, target, corr_limit=0.70, verbose=2, sep=',', \n",
            "\t\theader=0, test_data='',feature_engg='', category_encoders='',\n",
            "\t\tdask_xgboost_flag=False, nrows=None)\n",
            "Create new features via 'feature_engg' flag : ['interactions','groupby','target']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from featurewiz import FeatureWiz\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tscv import GapRollForward\n",
        "import neptune\n",
        "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f20JkWQXu28"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "workbook = gc.open('all_etfs_OHLCV')\n",
        "sheet_titles = []\n",
        "for sheet in workbook.worksheets():\n",
        "  sheet_titles.append(sheet.title)\n",
        "\n",
        "dict_of_sheets = {}\n",
        "for sheet_title in sheet_titles:\n",
        "  sheet = workbook.worksheet(sheet_title)\n",
        "  values = sheet.get_all_values()\n",
        "  dict_of_sheets[sheet_title] = values\n",
        "\n",
        "keys = list(dict_of_sheets)\n",
        "etfs = {}\n",
        "for etf in keys:\n",
        "  etfs[etf] = pd.read_csv(f'/content/drive/MyDrive/Bocci_Machine_Learning_Returns/Data/{etf}.csv')\n",
        "  etfs[etf]['date'] = pd.to_datetime(etfs[etf]['date'], format=\"%Y/%m/%d\")\n",
        "  etfs[etf].set_index('date', inplace=True)\n",
        "\n",
        "results_path='/content/drive/MyDrive/Bocci_Machine_Learning_Returns/Data/results_lstm.csv'\n",
        "results=pd.read_csv(results_path)\n",
        "results['id'] = results.etf + results.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYKqau-iYX2B"
      },
      "outputs": [],
      "source": [
        "def split_scale(df):\n",
        "  global y_train, y_pred, y_test, x_train, x_test, x_train_scaled, x_train_norm, x_test_scaled, x_test_norm, split_point, dates_df, dates_test_df, dates_train_df\n",
        "  #Exclude dependent and date\n",
        "  feature_names = df.columns\n",
        "  feature_names = feature_names.drop(['day', 'month', 'log_returns'])\n",
        "\n",
        "  #X, Y Split\n",
        "  x = df.loc[:, feature_names]\n",
        "  y = df.loc[:, 'log_returns']\n",
        "\n",
        "  #Train-Test Split\n",
        "  last_date = max(df.index)\n",
        "  split_point = pd.to_datetime((last_date - relativedelta(years = 3)).date())\n",
        "  x_test, y_test = x.loc[x.index >= split_point].values, y.loc[y.index >= split_point].values\n",
        "  train_length = len(x_test) * 4\n",
        "  x_train, y_train = x.loc[x.index < split_point].tail(train_length).values, y.loc[y.index < split_point].tail(train_length).values\n",
        "\n",
        "  #Scaling\n",
        "  scaler = preprocessing.StandardScaler().fit(x_train)\n",
        "  #normalizer = preprocessing.Normalizer().fit(x_train)\n",
        "  x_train_scaled = scaler.transform(x_train)\n",
        "  x_test_scaled = scaler.transform(x_test)\n",
        "  #x_train_norm = normalizer.transform(x_train)\n",
        "  #x_test_norm = normalizer.transform(x_test)\n",
        "\n",
        "  #Re-add column names\n",
        "  x_train_scaled = pd.DataFrame(x_train_scaled, columns = feature_names)\n",
        "  x_test_scaled = pd.DataFrame(x_test_scaled, columns = feature_names)\n",
        "  #x_train_norm = pd.DataFrame(x_train_norm, columns = feature_names)\n",
        "  #x_test_norm = pd.DataFrame(x_test_norm, columns = feature_names)\n",
        "  y_test = pd.DataFrame(y_test, columns=['Log_Returns'])\n",
        "  y_train= pd.DataFrame(y_train, columns=['Log_Returns'])\n",
        "\n",
        "  #Save dates\n",
        "  dates_df = pd.DataFrame(df.index)\n",
        "  dates_test_df = dates_df.loc[dates_df.date >= split_point].reset_index(drop=True)\n",
        "  dates_train_df = dates_df.loc[dates_df.date < split_point].tail(train_length).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vn_NHlY4Yk-5"
      },
      "outputs": [],
      "source": [
        "def featurewiz_selection():\n",
        "  train_features = x_train_scaled.copy()\n",
        "  test_features = x_test_scaled.copy()\n",
        "  train_labels = y_train.copy()\n",
        "  test_labels = y_test.copy()\n",
        "\n",
        "  features = FeatureWiz(corr_limit=0.70, feature_engg='', category_encoders='', dask_xgboost_flag=False, nrows=None, verbose=0)\n",
        "  train_features = features.fit_transform(train_features, train_labels)\n",
        "  test_features = features.transform(test_features)\n",
        "  return train_features, test_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_ff():\n",
        "  '''params = {\n",
        "        'learning_rate': [0.0005, 0.001],\n",
        "        'n_epochs': [100, 200],\n",
        "        'n_neurons': [8, 16, 50],\n",
        "        'n_hid_layers': [4, 12],\n",
        "        'dropout_rate': [0.1, 0.3],\n",
        "        'weight_decay': [0.00001, None],\n",
        "        'clipnorm': [1, None],\n",
        "        'earlystop': [True, False]\n",
        "    }'''\n",
        "\n",
        "  params = {\n",
        "        'learning_rate': 0.0005,\n",
        "        'n_epochs': 75,\n",
        "        'n_neurons': [16, 50],\n",
        "        'n_hid_layers': [4, 12],\n",
        "        'dropout_rate': 0.1,\n",
        "        'weight_decay': 0.00001,\n",
        "    }\n",
        "\n",
        "\n",
        "  train_features = tf.convert_to_tensor(x_train_scaled) \n",
        "  test_features = tf.convert_to_tensor(x_test_scaled) \n",
        "  train_labels = tf.convert_to_tensor(y_train) \n",
        "  test_labels = tf.convert_to_tensor(y_test)\n",
        "\n",
        "  def build_and_compile_ff(lr, clipping, decay, neurons, layers, dropout):\n",
        "    model_name='ff_nn'\n",
        "    model = Sequential()\n",
        "    for layer in range(0,layers):\n",
        "      model.add(Dense(neurons))\n",
        "      model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer=tf.keras.optimizers.Adam(weight_decay=decay, clipnorm=clipping, learning_rate=lr))\n",
        "\n",
        "    #shape = train_features.shape\n",
        "    #model.build(shape)\n",
        "    return model\n",
        "\n",
        "  #for a in range(0, len(params['clipnorm'])):\n",
        "    #for b in range(0, len(params['weight_decay'])):\n",
        "      #for c in range(0, len(params['dropout_rate'])):\n",
        "        #for d in range(0, len(params['learning_rate'])):\n",
        "  for e in range(0, len(params['n_hid_layers'])):\n",
        "    for f in range(0, len(params['n_neurons'])):\n",
        "  #for g in range(0, len(params['n_epochs'])):\n",
        "  #for h in range(0, len(params['earlystop'])):\n",
        "    #Start Run\n",
        "      run = neptune.init_run(\n",
        "          project=\"ku-master-research/master-thesis-ff\",\n",
        "          api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxM2Q3OTFlZi0zZWQ5LTRjY2UtOGUyMi1mOTY4ZWFkOWE4YTAifQ=='\n",
        "      )  # Credentials to log experiments\n",
        "      neptune_callback = NeptuneCallback(run=run)\n",
        "\n",
        "      #Get Params\n",
        "      #clipnorm = params['clipnorm'][a]\n",
        "      #weight_decay = params['weight_decay'][b]\n",
        "      #dropout_rate = params['dropout_rate'][c]\n",
        "      dropout_rate = params['dropout_rate']\n",
        "      #learning_rate = params['learning_rate'][d]\n",
        "      learning_rate = params['learning_rate']\n",
        "      n_hid_layers = params['n_hid_layers'][e]\n",
        "      n_neurons = params['n_neurons'][f]\n",
        "      #n_epochs = params['n_epochs'][g]\n",
        "      n_epochs = params['n_epochs']\n",
        "      #earlystop = params['earlystop'][h]\n",
        "\n",
        "      #Log Stuff\n",
        "      run['mytracking/etf'] = etf\n",
        "      #run['mytracking/learning_rate'] = learning_rate\n",
        "      #run['mytracking/dropout'] = dropout_rate\n",
        "      #run['mytracking/weight_decay'] = weight_decay\n",
        "      #run['mytracking/clipnorm'] = clipnorm\n",
        "      #run['mytracking/epochs'] = n_epochs\n",
        "      run['mytracking/n_neurons'] = n_neurons\n",
        "      run['mytracking/layers'] = n_hid_layers\n",
        "      #run['mytracking/earlystop'] = earlystop\n",
        "\n",
        "      ff = build_and_compile_ff(learning_rate, None, None, n_neurons, n_hid_layers, dropout_rate)\n",
        "      #ff = build_and_compile_ff(learning_rate, clipnorm, weight_decay, n_neurons, n_hid_layers, dropout_rate)\n",
        "      '''if earlystop:\n",
        "        es = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=10)\n",
        "        ff.fit(\n",
        "            train_features,\n",
        "            train_labels,\n",
        "            validation_split=0.2,\n",
        "            verbose=0,\n",
        "            epochs=n_epochs,\n",
        "            callbacks=[es, neptune_callback]\n",
        "        )\n",
        "      else:'''\n",
        "      ff.fit(\n",
        "          train_features,\n",
        "          train_labels,\n",
        "          validation_split=0.2,\n",
        "          verbose=0,\n",
        "          epochs=n_epochs,\n",
        "          callbacks=[neptune_callback]\n",
        "      )\n",
        "\n",
        "      #Make Predictions\n",
        "      y_pred = ff.predict(test_features)\n",
        "      run['mytracking/test_rmse'] = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "      run['mytracking/pred_stdev'] = np.std(y_pred)\n",
        "      run['mytracking/pred_min'] = np.min(y_pred)\n",
        "      run['mytracking/pred_max'] = np.max(y_pred)\n",
        "      run['mytracking/pred_mean'] = np.mean(y_pred)\n",
        "\n",
        "      #End Run\n",
        "      run.stop()"
      ],
      "metadata": {
        "id": "kfyouelm3cEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(window_size):\n",
        "  if window_size != 0:\n",
        "    n_future = 1\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "\n",
        "    #Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
        "    #Create windows for the training data\n",
        "    for i in range(window_size, len(x_train_scaled) - n_future +1):\n",
        "        train_features.append(x_train_scaled.iloc[i - window_size:i, 0:x_train_scaled.shape[1]])\n",
        "        train_labels.append(y_train.iloc[i + n_future - 1:i + n_future, 0])\n",
        "    train_features, train_labels = np.array(train_features), np.array(train_labels)\n",
        "\n",
        "    #Add the periods of the training data that are part of the required window length to the test data\n",
        "    x_test_lstm = pd.concat([x_train_scaled.iloc[len(x_train_scaled)-window_size:,:], x_test_scaled]).reset_index(drop=True)\n",
        "\n",
        "    #Create windows for the testing data\n",
        "    for i in range(window_size, len(x_test_lstm) - n_future +1):\n",
        "        test_features.append(x_test_lstm.iloc[i - window_size:i, 0:x_test_lstm.shape[1]])\n",
        "        test_labels.append(y_test.iloc[i-window_size: i- window_size+1, 0])\n",
        "    #Convert inputs to Tensors\n",
        "    test_features, test_labels = np.array(test_features), np.array(test_labels)\n",
        "    train_features = tf.convert_to_tensor(train_features) #train_features, scaled\n",
        "    test_features = tf.convert_to_tensor(test_features) #test_features, scaled\n",
        "    train_labels = tf.convert_to_tensor(train_labels) #train_labels\n",
        "    test_labels = tf.convert_to_tensor(test_labels) #test_labels\n",
        "    return train_features, test_features, train_labels, test_labels\n",
        "  else:\n",
        "    from tensorflow.python.ops.numpy_ops import np_config\n",
        "    np_config.enable_numpy_behavior()\n",
        "    train_features = tf.convert_to_tensor(x_train_scaled) #train_features, scaled\n",
        "    train_features = train_features.reshape((train_features.shape[0], train_features.shape[1], 1))\n",
        "    test_features = tf.convert_to_tensor(x_test_scaled) #test_features, scaled\n",
        "    test_features = test_features.reshape((test_features.shape[0], test_features.shape[1], 1))\n",
        "    train_labels = tf.convert_to_tensor(y_train) #train_labels\n",
        "    test_labels = tf.convert_to_tensor(y_test) #test_labels\n",
        "    return train_features, test_features, train_labels, test_labels\n",
        "  \n",
        "\n",
        "def build_lstm(n_hid_layers, n_units, decay, clipping, lr):\n",
        "  model_name='lstm_nn'\n",
        "  #Build and compile the model\n",
        "  lstm = Sequential()\n",
        "  for layer in range(0,n_hid_layers):\n",
        "    lstm.add(LSTM(n_units, return_sequences=True))\n",
        "    lstm.add(Dropout(0.05))\n",
        "  lstm.add(LSTM(n_units, return_sequences=False))\n",
        "  lstm.add(Dense(1))\n",
        "  lstm.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(weight_decay=decay, clipnorm=clipping, learning_rate=lr))\n",
        "  return lstm\n",
        "\n",
        "def validate_lstm():\n",
        "  #es = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=10)\n",
        "  '''params = {\n",
        "      'window_size': [0],\n",
        "      'n_epochs': [200, 400],\n",
        "      'n_units': [120, 200],\n",
        "      'n_hid_layers': [6, 10],\n",
        "      'val_dropout': [0.05, 0.1],\n",
        "      'weight_decay': [0, 0.00001],\n",
        "      'clipnorm': [1]\n",
        "  }\n",
        "  '''\n",
        "  params = {\n",
        "      'learning_rate': 0.0005,\n",
        "      'window_size': [0],\n",
        "      'n_epochs': [1000],\n",
        "      'n_units': [250],\n",
        "      'n_hid_layers': [10],\n",
        "      'val_dropout': [0.1],\n",
        "      'weight_decay': [0.00001],\n",
        "      'clipnorm': [1]\n",
        "  }\n",
        " \n",
        "  for a in range(0, len(params['window_size'])):\n",
        "    for b in range(0, len(params['n_epochs'])):\n",
        "      for c in range(0, len(params['n_units'])):\n",
        "        for d in range(0, len(params['n_hid_layers'])):\n",
        "          for e in range(0, len(params['val_dropout'])):\n",
        "            for f in range(0, len(params['weight_decay'])):\n",
        "              for g in range(0, len(params['clipnorm'])):\n",
        "                #Start Run\n",
        "                run = neptune.init_run(\n",
        "                    project=\"ku-master-research/master-thesis-lstm\",\n",
        "                    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxM2Q3OTFlZi0zZWQ5LTRjY2UtOGUyMi1mOTY4ZWFkOWE4YTAifQ==\",\n",
        "                )  # Credentials to log experiments\n",
        "                neptune_callback = NeptuneCallback(run=run)\n",
        "\n",
        "                #Get Params\n",
        "                learning_rate = params['learning_rate']\n",
        "                window_size = params['window_size'][a]\n",
        "                n_epochs = params['n_epochs'][b]\n",
        "                n_units = params['n_units'][c]\n",
        "                n_hid_layers = params['n_hid_layers'][d]\n",
        "                val_dropout = params['val_dropout'][e]\n",
        "                weight_decay = params['weight_decay'][f]\n",
        "                clipnorm = params['clipnorm'][g]\n",
        "\n",
        "                #Prepare Data into LSTM Format\n",
        "                train_features, test_features, train_labels, test_labels = prepare_data(window_size)\n",
        "\n",
        "                #Log Stuff\n",
        "                run['namespace/field_name'] = f'etf_{etf}_win_{window_size}_epo_{n_epochs}_uni_{n_units}_lay_{n_hid_layers}'\n",
        "                run['mytracking/etf'] = etf\n",
        "                run['mytracking/dropout'] = val_dropout\n",
        "                run['mytracking/weight_decay'] = weight_decay\n",
        "                run['mytracking/clipnorm'] = clipnorm\n",
        "                run['mytracking/window_size'] = window_size\n",
        "                run['mytracking/epochs'] = n_epochs\n",
        "                run['mytracking/units'] = n_units\n",
        "                run['mytracking/layers'] = n_hid_layers\n",
        "\n",
        "                #Build Model\n",
        "                lstm = build_lstm(n_hid_layers, n_units, weight_decay, clipnorm, learning_rate)\n",
        "\n",
        "                #Run Model\n",
        "                lstm.fit(\n",
        "                      train_features,\n",
        "                      train_labels,\n",
        "                      validation_split=0.2,\n",
        "                      verbose=0,\n",
        "                      epochs=n_epochs,\n",
        "                      #callbacks=[es, neptune_callback]\n",
        "                      callbacks=[neptune_callback]\n",
        "                  )\n",
        "                \n",
        "                #Make Predictions\n",
        "                y_pred = lstm.predict(test_features)\n",
        "                run['mytracking/test_rmse'] = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "                run['mytracking/pred_stdev'] = np.std(y_pred)\n",
        "                run['mytracking/pred_min'] = np.min(y_pred)\n",
        "                run['mytracking/pred_max'] = np.max(y_pred)\n",
        "                run['mytracking/pred_mean'] = np.mean(y_pred)\n",
        "\n",
        "                #End Run\n",
        "                run.stop()"
      ],
      "metadata": {
        "id": "vqSWpqW4q42T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for etf in keys:\n",
        "    split_scale(etfs[etf])\n",
        "    x_train_scaled, x_test_scaled = featurewiz_selection()\n",
        "    #validate_lstm()\n",
        "    validate_ff()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV67eKgu1zOK",
        "outputId": "e5b8601b-7df7-4ae7-dbb0-f23b9ffca676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 84)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 83 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (42) highly correlated variables:\n",
            "Completed SULOV. 41 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 41 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 41 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 21 important features:\n",
            "['outlier', 'sma_cross', 'trima_cross', 'hull_fast_cross', 'd_dema_200', 'dema_cross', 'd_sma_200', 'ema_cross', 'd_natr', 'macd_hist_cross', 'aro', 'd_open', 'hull_slow_cross', 'd_maeu', 'ppo_hist', 'diminus', 'd_adx', 'psar_hist', 'd_pvi', 'd_bold', 'cfo']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 21 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-225\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 32 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 32 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-225/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-226\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 29 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 29 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-226/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-227\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 11 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 11 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-227/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-228\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 44 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 44 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-228/metadata\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 83)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 82 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (39) highly correlated variables:\n",
            "Completed SULOV. 43 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 43 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 43 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 23 important features:\n",
            "['outlier', 'ema_cross', 'rsi', 'fi', 'd_vwma', 'd_wma_200', 'year', 'd_high', 'cfo', 'macd_hist_cross', 'd_pvi', 'ppo_hist', 'psar_cross', 'hull_slow_cross', 'd_wma_50', 'month_sin', 'psar_hist', 'log_d_nvi', 'd_bol_width', 'arp', 'pvo_hist', 'd_atr', 'arn']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 23 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-229\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 29 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 29 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-229/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-230\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 29 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 29 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-230/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-231\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 20 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 20 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-231/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-232\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 20 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 20 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-232/metadata\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 84)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 83 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (37) highly correlated variables:\n",
            "Completed SULOV. 46 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 46 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 46 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 21 important features:\n",
            "['outlier', 'd_kelu', 'dema_cross', 'd_trima_200', 'psar_cross', 'last_log_return', 'd_bolu', 'day_cos', 'd_open', 'hull_fast_cross', 'dmi', 'd_hull_12', 'apo', 'd_vwma', 'd_diminus', 'ppo_hist', 'month_sin', 'mi', 'd_dema_200', 'd_bold', 'stoch_d']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 21 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-233\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 32 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 32 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-233/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-234\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 32 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 32 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-234/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-235\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 11 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 11 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-235/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-236\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 11 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 11 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-236/metadata\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 84)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 83 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (40) highly correlated variables:\n",
            "Completed SULOV. 43 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 43 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 43 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 22 important features:\n",
            "['outlier', 'ema_cross', 'psar_cross', 'd_hull_26', 'wma_cross', 'eom', 'd_nvi', 'd_natr', 'dpo', 'mfi', 'aro', 'd_wma_200', 'last_log_return', 'macd_hist_cross', 'ppo_hist', 'hull_fast_cross', 'd_obv', 'month_sin', 'd_hull_12', 'year', 'd_trima_50', 'd_open']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 22 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-237\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 29 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 29 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-237/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-238\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 26 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-238/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-239\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 11 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 11 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-239/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-240\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 26 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-240/metadata\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 82)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 81 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (37) highly correlated variables:\n",
            "Completed SULOV. 44 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 44 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 44 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 20 important features:\n",
            "['outlier', 'hull_slow_cross', 'macd_hist_cross', 'sma_cross', 'apo', 'd_ema_26', 'psar_hist', 'd_sma_200', 'pvo_hist', 'd_bolu', 'day_sin', 'd_natr', 'd_open', 'd_hull_200', 'arp', 'd_obv', 'ppo_hist', 'd_diplus', 'last_log_return', 'bol_pct']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 20 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-241\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 41 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 41 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-241/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-242\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 32 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 32 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-242/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-243\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 14 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-243/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-244\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 14 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-244/metadata\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 84)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 83 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (40) highly correlated variables:\n",
            "Completed SULOV. 43 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 43 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 43 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 20 important features:\n",
            "['outlier', 'macd_hist_cross', 'd_wma_50', 'aro', 'd_hull_12', 'd_wma_200', 'd_low', 'd_hull_26', 'ppo_hist', 'cfo', 'd_bol_width', 'sma_cross', 'dema_cross', 'ema_cross', 'd_pvi', 'd_natr', 'psar_cross', 'd_diplus', 'year', 'trima_cross']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 20 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-245\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 29 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 29 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-245/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-246\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 26 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-246/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-247\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 8 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 8 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-247/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-248\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 8 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 8 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-248/metadata\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 79)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 78 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (34) highly correlated variables:\n",
            "Completed SULOV. 44 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 44 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 44 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 21 important features:\n",
            "['outlier', 'trima_cross', 'd_volume', 'd_low', 'hull_fast_cross', 'd_obv', 'ema_cross', 'cfo', 'd_bol_width', 'd_hull_200', 'd_psar_hist', 'd_stoch_k', 'log_d_ema_26', 'macd_hist', 'd_arp', 'd_williams_r', 'sma_cross', 'd_atr', 'd_pvi', 'log_d_abau', 'd_arn']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 21 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-249\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 26 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-249/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-250\n",
            "25/25 [==============================] - 0s 1ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 50 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 50 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-250/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-251\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 20 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 20 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-251/metadata\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-252\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 17 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/ku-master-research/master-thesis-ff/e/MFF-252/metadata\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+XU5rEIY9h/fQ88XNY3/f",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}