{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardobocci/ml-stock-market/blob/main/1.master_thesis_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8n19scrRGk-"
      },
      "source": [
        "Change FFNN to same style as LSTM which allows defining num_layers\n",
        "\n",
        "Add cross validation to lstm and ffnn parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUlXOw6xg3Ip"
      },
      "source": [
        "Pipeline Steps:\n",
        "7. Load data for selected ETF\n",
        "8. Split into train/test\n",
        "9. Normalise scales/distributions\n",
        "10. Feature selection\n",
        "11. Run Models\n",
        "12. Add Model Results to Results File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ7G6CjJUtyB"
      },
      "source": [
        "# Libraries and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlgOYym69hfX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install feature_engine\n",
        "!pip install featurewiz==0.1.996\n",
        "!pip install tscv\n",
        "!pip install lightgbm\n",
        "!pip install pmdarima --upgrade\n",
        "!pip install sklearn-genetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXPESMBBArpO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from sklearn import metrics\n",
        "from genetic_selection import GeneticSelectionCV\n",
        "from tscv import GapRollForward\n",
        "from tensorflow import keras\n",
        "\n",
        "import pmdarima as pm\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRjjJ4zS9IfF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "workbook = gc.open('all_etfs_OHLCV')\n",
        "sheet_titles = []\n",
        "for sheet in workbook.worksheets():\n",
        "  sheet_titles.append(sheet.title)\n",
        "\n",
        "dict_of_sheets = {}\n",
        "for sheet_title in sheet_titles:\n",
        "  sheet = workbook.worksheet(sheet_title)\n",
        "  values = sheet.get_all_values()\n",
        "  dict_of_sheets[sheet_title] = values\n",
        "\n",
        "keys = list(dict_of_sheets)\n",
        "etfs = {}\n",
        "for etf in keys:\n",
        "  etfs[etf] = pd.read_csv(f'/content/drive/MyDrive/Bocci_Machine_Learning_Returns/Data/{etf}.csv')\n",
        "  etfs[etf]['date'] = pd.to_datetime(etfs[etf]['date'], format=\"%Y/%m/%d\")\n",
        "  etfs[etf].set_index('date', inplace=True)\n",
        "\n",
        "results_path='/content/drive/MyDrive/Bocci_Machine_Learning_Returns/Data/results.csv'\n",
        "results=pd.read_csv(results_path)\n",
        "results['id'] = results.etf + results.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9mF0t952Dp3"
      },
      "outputs": [],
      "source": [
        "#Import Models\n",
        "from featurewiz import FeatureWiz\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from lightgbm import LGBMRegressor\n",
        "from prophet import Prophet\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqp6WBYW6Y_U"
      },
      "source": [
        "#Split and Scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQN4yJ-xjSpC"
      },
      "source": [
        "Splitting into test/train. A validation set is not necessary as hyper-parameters are tuned using cross validation instead.\n",
        "\n",
        "The test set includes 3 years of data prior to the last available date. The train set is trimmed to obtain an 80:20 train:test split. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w894HfpNmLy1"
      },
      "source": [
        "Scaling the data to have a mean of 0 and a unit standard deviation. The scaling happens only based on the fitting done on the training set. Doing a fit transform on the test set, or scaling before splitting would cause spillage (using info fron the test set).\n",
        "\n",
        "Scaling was found to perform consistently and significantly better than normalising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijKyt4SgPoOX"
      },
      "outputs": [],
      "source": [
        "def split_scale(df):\n",
        "  global y_train, y_pred, y_test, x_train, x_test, x_train_scaled, x_train_norm, x_test_scaled, x_test_norm, split_point, dates_df, dates_test_df, dates_train_df\n",
        "  #Exclude dependent and date\n",
        "  feature_names = df.columns\n",
        "  feature_names = feature_names.drop(['day', 'month', 'log_returns'])\n",
        "\n",
        "  #X, Y Split\n",
        "  x = df.loc[:, feature_names]\n",
        "  y = df.loc[:, 'log_returns']\n",
        "\n",
        "  #Train-Test Split\n",
        "  last_date = max(df.index)\n",
        "  split_point = pd.to_datetime((last_date - relativedelta(years = 3)).date())\n",
        "  x_test, y_test = x.loc[x.index >= split_point].values, y.loc[y.index >= split_point].values\n",
        "  train_length = len(x_test) * 4\n",
        "  x_train, y_train = x.loc[x.index < split_point].tail(train_length).values, y.loc[y.index < split_point].tail(train_length).values\n",
        "\n",
        "  #Scaling\n",
        "  scaler = preprocessing.StandardScaler().fit(x_train)\n",
        "  #normalizer = preprocessing.Normalizer().fit(x_train)\n",
        "  x_train_scaled = scaler.transform(x_train)\n",
        "  x_test_scaled = scaler.transform(x_test)\n",
        "  #x_train_norm = normalizer.transform(x_train)\n",
        "  #x_test_norm = normalizer.transform(x_test)\n",
        "\n",
        "  #Re-add column names\n",
        "  x_train_scaled = pd.DataFrame(x_train_scaled, columns = feature_names)\n",
        "  x_test_scaled = pd.DataFrame(x_test_scaled, columns = feature_names)\n",
        "  #x_train_norm = pd.DataFrame(x_train_norm, columns = feature_names)\n",
        "  #x_test_norm = pd.DataFrame(x_test_norm, columns = feature_names)\n",
        "  y_test = pd.DataFrame(y_test, columns=['Log_Returns'])\n",
        "  y_train= pd.DataFrame(y_train, columns=['Log_Returns'])\n",
        "\n",
        "  #Save dates\n",
        "  dates_df = pd.DataFrame(df.index)\n",
        "  dates_test_df = dates_df.loc[dates_df.date >= split_point].reset_index(drop=True)\n",
        "  dates_train_df = dates_df.loc[dates_df.date < split_point].tail(train_length).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Selection"
      ],
      "metadata": {
        "id": "lLMsd2LBGWcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def genetic_selection(estimator):\n",
        "  train_features = x_train_scaled.copy()\n",
        "  test_features = x_test_scaled.copy()\n",
        "  train_labels = y_train.copy()\n",
        "  test_labels = y_test.copy()\n",
        "  start_period = y_test.index[0]\n",
        "  end_period = y_test.index[-1]\n",
        "  test_periods = len(y_test)\n",
        "  splitter_size = int(0.16*len(train_features))\n",
        "  splitter = GapRollForward(gap_size=0, min_test_size=splitter_size, min_train_size=splitter_size, max_test_size=splitter_size)\n",
        "\n",
        "  selector = GeneticSelectionCV(\n",
        "    estimator,\n",
        "    cv=splitter,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_population=100,\n",
        "    crossover_proba=0.5,\n",
        "    mutation_proba=0.2,\n",
        "    n_generations=40,\n",
        "    crossover_independent_proba=0.1,\n",
        "    mutation_independent_proba=0.05,\n",
        "    tournament_size=3,\n",
        "    n_gen_no_change=5,\n",
        "    n_jobs=1,\n",
        "  )\n",
        "  selector = selector.fit(train_features, train_labels.values.ravel())\n",
        "  \n",
        "  selected_features = train_features.columns[selector.support_]\n",
        "  train_features = train_features[selected_features]\n",
        "  test_features = test_features[selected_features]\n",
        "  return train_features, test_features"
      ],
      "metadata": {
        "id": "oJpEFyiDGUHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recursive_elimination(estimator):\n",
        "  train_features = x_train_scaled.copy()\n",
        "  test_features = x_test_scaled.copy()\n",
        "  train_labels = y_train.copy()\n",
        "  test_labels = y_test.copy()\n",
        "  \n",
        "  splitter_size = int(0.16*len(train_features))\n",
        "  splitter = GapRollForward(gap_size=0, min_test_size=splitter_size, min_train_size=splitter_size, max_test_size=splitter_size)\n",
        "  selector = RFECV(estimator, step=1, cv=splitter, scoring='neg_mean_squared_error')\n",
        "  selector.fit(train_features, train_labels.values.ravel())\n",
        "  selected_cols = selector.get_support()\n",
        "  train_features = train_features.iloc[:, selected_cols]\n",
        "  test_features = test_features.iloc[:, selected_cols]\n",
        "  return train_features, test_features"
      ],
      "metadata": {
        "id": "cEpEKEKne7zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def featurewiz_selection():\n",
        "  train_features = x_train_scaled.copy()\n",
        "  test_features = x_test_scaled.copy()\n",
        "  train_labels = y_train.copy()\n",
        "  test_labels = y_test.copy()\n",
        "\n",
        "  features = FeatureWiz(corr_limit=0.70, feature_engg='', category_encoders='', dask_xgboost_flag=False, nrows=None, verbose=0)\n",
        "  train_features = features.fit_transform(train_features, train_labels)\n",
        "  test_features = features.transform(test_features)\n",
        "  return train_features, test_features"
      ],
      "metadata": {
        "id": "2f-NqskeE-gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kGTNXsiONGE"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Useful Functions"
      ],
      "metadata": {
        "id": "M6h_GML728r8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK1qpkuTy954"
      },
      "outputs": [],
      "source": [
        "models = ['last_price', 'ols', 'ridge', 'lasso', 'elastic_net', 'decision_tree', 'random_forest', 'gradient_boost', 'xgboost', 'sv_rbf', 'lgbm', 'arima', 'ff_nn', 'lstm_nn', 'geometric_brownian']\n",
        "model_predictions = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(etf, model_name, y_test, y_pred):\n",
        "  path = results_path\n",
        "  model_results = pd.DataFrame({'etf':etf,\n",
        "                                  'model':model_name,\n",
        "                                  'rmse':np.sqrt(metrics.mean_squared_error(y_test, y_pred))}, index=[0])\n",
        "  results=pd.read_csv(path)\n",
        "  results['id'] = results.etf + results.model\n",
        "  res = results.copy()\n",
        "  res = pd.concat([res, model_results]).reset_index(drop=True)\n",
        "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "    res.to_csv(f, index=False)"
      ],
      "metadata": {
        "id": "oTVHp5_ly_yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_fits_predictions(model_predictions, model_fits, model_name, etf):\n",
        "  workbook = gc.open('predictions')\n",
        "  sheet_titles = []\n",
        "  for sheet in workbook.worksheets():\n",
        "    sheet_titles.append(sheet.title)\n",
        "\n",
        "  if etf in sheet_titles:\n",
        "    sheet = workbook.worksheet(etf)\n",
        "    predicted_df = pd.DataFrame(sheet.get_all_records())\n",
        "    predicted_df[f'{model_name}_predicted'] = pd.DataFrame(model_predictions)\n",
        "    predicted_df[f'{model_name}_error'] = predicted_df.true-predicted_df[f'{model_name}_predicted']\n",
        "    sheet.update([predicted_df.columns.values.tolist()] + predicted_df.values.tolist())\n",
        "  else:\n",
        "    predicted_df = pd.DataFrame()\n",
        "    predicted_df['date']=dates_test_df['date'].astype(str)\n",
        "    predicted_df['true'] = pd.DataFrame(y_test)\n",
        "    predicted_df[f'{model_name}_predicted'] = pd.DataFrame(model_predictions)\n",
        "    predicted_df[f'{model_name}_error'] = predicted_df.true-predicted_df[f'{model_name}_predicted']\n",
        "    sheet = workbook.add_worksheet(etf, predicted_df.shape[0], predicted_df.shape[1])\n",
        "    sheet.update([predicted_df.columns.values.tolist()] + predicted_df.values.tolist())\n",
        "\n",
        "  workbook = gc.open('fits')\n",
        "  sheet_titles = []\n",
        "  for sheet in workbook.worksheets():\n",
        "    sheet_titles.append(sheet.title)\n",
        "\n",
        "  if etf in sheet_titles:\n",
        "    sheet = workbook.worksheet(etf)\n",
        "    fitted_df = pd.DataFrame(sheet.get_all_records())\n",
        "    fitted_df[f'{model_name}_fitted'] = pd.DataFrame(model_fits)\n",
        "    fitted_df[f'{model_name}_error'] = fitted_df.true-fitted_df[f'{model_name}_fitted']\n",
        "    sheet.update([fitted_df.columns.values.tolist()] + fitted_df.values.tolist())\n",
        "  else:\n",
        "    fitted_df = pd.DataFrame()\n",
        "    fitted_df['date']=dates_train_df['date'].astype(str)\n",
        "    fitted_df['true'] = pd.DataFrame(y_train)\n",
        "    fitted_df[f'{model_name}_fitted'] = pd.DataFrame(model_fits)\n",
        "    fitted_df[f'{model_name}_error'] = fitted_df.true-fitted_df[f'{model_name}_fitted']\n",
        "    sheet = workbook.add_worksheet(etf, fitted_df.shape[0], fitted_df.shape[1])\n",
        "    sheet.update([fitted_df.columns.values.tolist()] + fitted_df.values.tolist())\n",
        "  "
      ],
      "metadata": {
        "id": "S3V1DEgvmg4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_params(model, model_name, etf):\n",
        "  params = model.get_params()\n",
        "  params = pd.DataFrame(params, index=[0])\n",
        "  params['etf'] = etf\n",
        "  params = params.fillna('')\n",
        "\n",
        "  workbook = gc.open('params')\n",
        "  sheet_titles = []\n",
        "  for sheet in workbook.worksheets():\n",
        "    sheet_titles.append(sheet.title)\n",
        "\n",
        "  if model_name in sheet_titles:\n",
        "    sheet = workbook.worksheet(f'{model_name}')\n",
        "    params_all = pd.DataFrame(sheet.get_all_records())\n",
        "    params_all = pd.concat([params_all, params]).reset_index(drop=True)\n",
        "    sheet.update([params_all.columns.values.tolist()] + params_all.values.tolist())\n",
        "\n",
        "  else:\n",
        "    sheet = workbook.add_worksheet(model_name, params.shape[0], params.shape[1])\n",
        "    sheet.update([params.columns.values.tolist()] + params.values.tolist())"
      ],
      "metadata": {
        "id": "rhYQvIyMMuEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_selected_cols(x_train_scaled, model_name, etf):\n",
        "  cols = x_train_scaled.columns\n",
        "  cols = pd.DataFrame(cols)\n",
        "  cols.columns = ['features']\n",
        "  cols['etf'] = etf\n",
        "  cols['model'] = model_name\n",
        "\n",
        "  workbook = gc.open('features')\n",
        "  sheet=workbook.worksheet('Sheet1')\n",
        "  features_df = pd.DataFrame(sheet.get_all_records())\n",
        "\n",
        "  features_df = pd.concat([cols, features_df])\n",
        "  sheet.update([features_df.columns.values.tolist()] + features_df.values.tolist())"
      ],
      "metadata": {
        "id": "H6L94DY5fp7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cross_val(estimator, parameters):\n",
        "  splitter_size = int(0.16*len(x_train_scaled))\n",
        "  splitter = GapRollForward(gap_size=0, min_test_size=splitter_size, min_train_size=splitter_size, max_test_size=splitter_size)\n",
        "  cv = GridSearchCV(estimator=estimator, param_grid=parameters, cv=splitter, scoring='neg_mean_squared_error')\n",
        "  cv.fit(x_train_scaled, y_train.values.ravel())\n",
        "  params = pd.DataFrame(cv.best_params_, index=[0])\n",
        "  return params"
      ],
      "metadata": {
        "id": "vsAtFe6I27Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzGJHoBsg53_"
      },
      "source": [
        "##Return=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy-0turfhCVQ"
      },
      "source": [
        "Models need to beat a simple returns=0 benchmark, where returns are assumed to follow a random walk with no drift."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBTHUAH_g-2S"
      },
      "outputs": [],
      "source": [
        "def run_last_price():\n",
        "  model_name='last_price'\n",
        "  y_pred = y_test.copy()\n",
        "  y_fit = y_train.copy()\n",
        "  y_pred.Log_Returns = 0\n",
        "  y_fit.Log_Returns = 0\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Geometric Brownian Motion"
      ],
      "metadata": {
        "id": "4amT34XWhPpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_geom_brown():  \n",
        "  model_name = 'geometric_brownian'\n",
        "  #Starting Price\n",
        "  So = 1000\n",
        "  #Time increment\n",
        "  dt = 1\n",
        "  T = len(y_test)\n",
        "  T_train = len(y_train)\n",
        "  N = T / dt\n",
        "  N_train = T_train / dt\n",
        "  #Time array\n",
        "  t = np.arange(1, int(N) + 1)\n",
        "  t_train = np.arange(1, int(N_train) + 1)\n",
        "  #Historical Mean Returns\n",
        "  mu = np.mean(y_train)[0]\n",
        "  mu_train = 0\n",
        "  #Historical volatility\n",
        "  sigma = np.std(y_train)[0]\n",
        "  sigma_train = 0.1\n",
        "  #Number of simulations\n",
        "  scen_size = 500\n",
        "  #Run simulation\n",
        "  b = {str(scen): np.random.normal(0, 1, int(N)) for scen in range(1, scen_size + 1)}\n",
        "  b_train = {str(scen): np.random.normal(0, 1, int(N_train)) for scen in range(1, scen_size + 1)}\n",
        "  #Brownian Path\n",
        "  W = {str(scen): b[str(scen)].cumsum() for scen in range(1, scen_size + 1)}\n",
        "  W_train = {str(scen): b_train[str(scen)].cumsum() for scen in range(1, scen_size + 1)}\n",
        "  #Drift\n",
        "  drift = (mu - 0.5 * sigma**2) * t\n",
        "  drift_train = (mu_train - 0.5 * sigma_train**2) * t_train\n",
        "  #Diffusion\n",
        "  diffusion = {str(scen): sigma * W[str(scen)] for scen in range(1, scen_size + 1)}\n",
        "  diffusion_train = {str(scen): sigma_train * W_train[str(scen)] for scen in range(1, scen_size + 1)}\n",
        "  #Predicted Scenarios\n",
        "  S = np.array([So * np.exp(drift + diffusion[str(scen)]) for scen in range(1, scen_size + 1)]) \n",
        "  S = np.hstack((np.array([[So] for scen in range(scen_size)]), S))\n",
        "  S_train = np.array([So * np.exp(drift_train + diffusion_train[str(scen)]) for scen in range(1, scen_size + 1)]) \n",
        "  S_train = np.hstack((np.array([[So] for scen in range(scen_size)]), S_train))\n",
        "  df = pd.DataFrame(S.T)\n",
        "  df_train = pd.DataFrame(S_train.T)\n",
        "  #Calculate Returns\n",
        "  rets_df = pd.DataFrame()\n",
        "  rets_df_train = pd.DataFrame()\n",
        "  for i in range (0, len(df.columns)):\n",
        "    rets_df[f'log_ret_{i}'] = np.log(df[i]) - np.log(df[i].shift(1))\n",
        "  for i in range (0, len(df_train.columns)):\n",
        "    rets_df_train[f'log_ret_{i}'] = np.log(df_train[i]) - np.log(df_train[i].shift(1))\n",
        "  rets_df = rets_df.dropna().reset_index(drop=True)\n",
        "  rets_df_train = rets_df_train.dropna().reset_index(drop=True)\n",
        "  y_pred = pd.DataFrame(rets_df.mean(axis=1), columns=['Log_Returns'])\n",
        "  y_fit = pd.DataFrame(rets_df_train.mean(axis=1), columns=['Log_Returns'])\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)"
      ],
      "metadata": {
        "id": "voGuZQG4hOef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ66lLU_Oc4I"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxw7vkBOeOtT"
      },
      "outputs": [],
      "source": [
        "def run_ols():\n",
        "  model_name='ols'\n",
        "  ols = LinearRegression().fit(x_train_scaled, y_train)\n",
        "  y_pred = ols.predict(x_test_scaled)\n",
        "  y_fit = ols.predict(x_train_scaled)\n",
        "  save_params(model=ols, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)\n",
        "  save_selected_cols(x_train_scaled=x_train_scaled, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXjYlD0Z3uTt"
      },
      "source": [
        "## Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXOnF09K372o"
      },
      "outputs": [],
      "source": [
        "def run_ridge():\n",
        "  model_name='ridge'\n",
        "\n",
        "  ridge = Ridge()\n",
        "  parameters = {\n",
        "      'alpha': [0.1, 1, 10, 100]\n",
        "  }\n",
        "  params = run_cross_val(ridge, parameters)\n",
        "\n",
        "  ridge = Ridge(alpha=params.loc[0, 'alpha']).fit(x_train_scaled, y_train)\n",
        "  y_pred = ridge.predict(x_test_scaled)\n",
        "  y_fit = ridge.predict(x_train_scaled)\n",
        "  \n",
        "  save_params(model=ridge, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)\n",
        "  save_selected_cols(x_train_scaled=x_train_scaled, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBU8Va28YupO"
      },
      "source": [
        "## Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lB6R7ngYzCh"
      },
      "outputs": [],
      "source": [
        "def run_lasso():\n",
        "  model_name='lasso'\n",
        "\n",
        "  las = Lasso()\n",
        "  parameters = {\n",
        "      'alpha': [0.01, 1, 100, 100]\n",
        "  }\n",
        "  params = run_cross_val(las, parameters)\n",
        "\n",
        "  las = Lasso(alpha=params.loc[0, 'alpha']).fit(x_train_scaled, y_train)\n",
        "  y_pred = las.predict(x_test_scaled)\n",
        "  y_fit = las.predict(x_train_scaled)\n",
        "\n",
        "  save_params(model=las, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl0nxGzAZpC-"
      },
      "source": [
        "## Elastic Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn-f8ycsZm3P"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "def run_elastic_net():\n",
        "  model_name='elastic_net'\n",
        "\n",
        "  elnet = ElasticNet(max_iter=10000)\n",
        "  parameters = {\n",
        "      'l1_ratio': [0.3, 0.5, 0.7],\n",
        "      'alpha': [0.01, 0.5, 1, 100]\n",
        "  }\n",
        "  params = run_cross_val(elnet, parameters)\n",
        "\n",
        "  elnet = ElasticNet(l1_ratio=params.loc[0, 'l1_ratio'], max_iter=10000)\n",
        "  elnet.fit(x_train_scaled, y_train.values.ravel())\n",
        "  y_pred = elnet.predict(x_test_scaled)\n",
        "  y_fit = elnet.predict(x_train_scaled)\n",
        "\n",
        "  save_params(model=elnet, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)\n",
        "  save_selected_cols(x_train_scaled=x_train_scaled, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0VM0Tl3dAlP"
      },
      "source": [
        "## Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQpfWkE5c_r5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "def run_decision_tree():\n",
        "  model_name='decision_tree'\n",
        "\n",
        "  tree = DecisionTreeRegressor(random_state=0)\n",
        "  parameters = {\n",
        "      'max_depth': [2, 5, 10, 50],\n",
        "      'min_samples_leaf': [5, 10, 20]\n",
        "  }\n",
        "  params = run_cross_val(tree, parameters)\n",
        "\n",
        "  tree = DecisionTreeRegressor(random_state=0, max_depth=params.loc[0, 'max_depth'], min_samples_leaf=params.loc[0, 'min_samples_leaf'])\n",
        "  tree.fit(x_train_scaled, y_train)\n",
        "  y_pred = tree.predict(x_test_scaled)\n",
        "  y_fit = tree.predict(x_train_scaled)\n",
        "\n",
        "  save_params(model=tree, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)\n",
        "  save_selected_cols(x_train_scaled=x_train_scaled, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHMFqN0NqIxJ"
      },
      "source": [
        "## Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VetAGQ4ZqIF2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "def run_random_forest():\n",
        "  model_name='random_forest'\n",
        "\n",
        "  forest = RandomForestRegressor(random_state=0)\n",
        "  parameters = {\n",
        "      'n_estimators': [8, 64, 128],\n",
        "      'max_depth': [2, 5, 10, 50],\n",
        "      'min_samples_leaf': [5, 10, 20]\n",
        "  }\n",
        "  params = run_cross_val(forest, parameters)\n",
        "\n",
        "  forest = RandomForestRegressor(random_state=0, max_depth=params.loc[0, 'max_depth'], min_samples_leaf=params.loc[0, 'min_samples_leaf'], n_estimators=params.loc[0, 'n_estimators'])\n",
        "  forest.fit(x_train_scaled, y_train.values.ravel())\n",
        "  y_pred = forest.predict(x_test_scaled)\n",
        "  y_fit = forest.predict(x_train_scaled)\n",
        "\n",
        "  save_params(model=forest, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)\n",
        "  save_selected_cols(x_train_scaled=x_train_scaled, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciZBSrfyzHpC"
      },
      "source": [
        "## Gradient Boosting Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9dAZ_wrzL5c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "def run_gradient_boost():\n",
        "  model_name='gradient_boost'\n",
        "\n",
        "  gboost = GradientBoostingRegressor(random_state=0)\n",
        "  parameters = {\n",
        "      'learning_rate': [0.025, 0.05],\n",
        "      'n_estimators': [15, 30, 50],\n",
        "      'max_depth': [2, 5, 10],\n",
        "      'min_samples_leaf': [5, 15]\n",
        "  }\n",
        "  params = run_cross_val(gboost, parameters)\n",
        "\n",
        "  gboost = GradientBoostingRegressor(random_state=0, max_depth=params.loc[0, 'max_depth'], min_samples_leaf=params.loc[0, 'min_samples_leaf'], n_estimators=params.loc[0, 'n_estimators'],learning_rate=params.loc[0, 'learning_rate'])\n",
        "  gboost.fit(x_train_scaled, y_train.values.ravel())\n",
        "  y_pred = gboost.predict(x_test_scaled)\n",
        "  y_fit = gboost.predict(x_train_scaled)\n",
        "\n",
        "  save_params(model=gboost, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)\n",
        "  save_selected_cols(x_train_scaled=x_train_scaled, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGVH88lyNlHZ"
      },
      "source": [
        "## Extreme Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h03F1VjQ6vbM"
      },
      "outputs": [],
      "source": [
        "def run_xgboost():\n",
        "  model_name='xgboost'\n",
        "  exgboost = XGBRegressor(random_state=0, objective = 'reg:squarederror')\n",
        "  parameters = {\n",
        "      'learning_rate': [0.08, 0.1, 0.12],\n",
        "      'n_estimators': [50, 80, 120],\n",
        "      'max_depth': [2, 5, 10],\n",
        "  }\n",
        "  params = run_cross_val(exgboost, parameters)\n",
        "\n",
        "  exgboost = XGBRegressor(random_state=0, objective = 'reg:squarederror', learning_rate=params.loc[0, 'learning_rate'], max_depth=params.loc[0, 'max_depth'], n_estimators=params.loc[0, 'n_estimators'])\n",
        "  exgboost.fit(x_train_scaled, y_train.values.ravel())\n",
        "  y_pred = exgboost.predict(x_test_scaled)\n",
        "  y_fit = exgboost.predict(x_train_scaled)\n",
        "\n",
        "  save_params(model=exgboost, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)\n",
        "  save_selected_cols(x_train_scaled=x_train_scaled, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DigS7uOcBYYB"
      },
      "source": [
        "## Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsX1DlEZBd3T"
      },
      "outputs": [],
      "source": [
        "def run_sv_rbf():\n",
        "  model_name='sv_rbf'\n",
        "  svr = SVR(kernel=\"rbf\")\n",
        "  parameters = {\n",
        "        'epsilon': [0.01, 0.05, 0.1],\n",
        "        'gamma': ['scale', 'auto'],\n",
        "        'C': [0.1, 1, 100]\n",
        "    }\n",
        "  params = run_cross_val(svr, parameters)\n",
        "\n",
        "  svr = SVR(kernel=\"rbf\", epsilon=params.loc[0, 'epsilon'], gamma=params.loc[0, 'gamma'],C=params.loc[0, 'C'])\n",
        "  svr.fit(x_train_scaled, y_train.values.ravel())\n",
        "  y_pred = svr.predict(x_test_scaled)\n",
        "  y_fit = svr.predict(x_train_scaled)\n",
        "\n",
        "  save_params(model=svr, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)\n",
        "  save_selected_cols(x_train_scaled=x_train_scaled, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuaVI0XJpmdW"
      },
      "source": [
        "##LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSY5biNZpp5R"
      },
      "outputs": [],
      "source": [
        "def run_lgbm():\n",
        "  model_name='lgbm'\n",
        "  lgbm = LGBMRegressor()\n",
        "  parameters = {\n",
        "      'learning_rate': [0.05, 0.1, 0.2],\n",
        "      'n_estimators': [15, 30, 50, 100],\n",
        "      'max_depth': [-1, 2, 5, 10]\n",
        "  }\n",
        "  params = run_cross_val(lgbm, parameters)\n",
        "\n",
        "  lgbm = LGBMRegressor(random_state=0, max_depth=params.loc[0, 'max_depth'], n_estimators=params.loc[0, 'n_estimators'],learning_rate=params.loc[0, 'learning_rate'])\n",
        "  lgbm.fit(x_train_scaled, y_train.values.ravel())\n",
        "  y_pred = lgbm.predict(x_test_scaled)\n",
        "  y_fit = lgbm.predict(x_train_scaled)\n",
        "\n",
        "  save_params(model=lgbm, model_name=model_name, etf=etf)\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)\n",
        "  save_selected_cols(x_train_scaled=x_train_scaled, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbNCyypYjsml"
      },
      "source": [
        "## ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9TYFvsGjt9J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "def run_arima():\n",
        "  model_name='arima'\n",
        "  train_features = x_train_scaled.copy() \n",
        "  test_features = x_test_scaled.copy()\n",
        "  train_labels = y_train.copy()\n",
        "  test_labels = y_test.copy()\n",
        "  test_periods = len(test_labels)\n",
        "  train_periods = len(train_labels)\n",
        "\n",
        "  arimax = pm.auto_arima(y=train_labels, X=train_features, seasonal=False, stationary=True, information_criterion='bic')\n",
        "  y_pred = arimax.predict(X=test_features, n_periods=test_periods)\n",
        "  y_fit = arimax.predict(X=train_features, n_periods=train_periods)\n",
        "  y_fit = pd.DataFrame(y_fit).to_numpy()\n",
        "  y_pred = pd.DataFrame(y_pred).to_numpy()\n",
        "\n",
        "  save_results(etf=etf, model_name=model_name, y_test=test_labels, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Networks"
      ],
      "metadata": {
        "id": "5l98m3FVqOdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feed Forward NN (Sequential)"
      ],
      "metadata": {
        "id": "NYc-fsL3kut2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ff_nn():\n",
        "  model_name='ff_nn'\n",
        "  train_features = tf.convert_to_tensor(x_train_scaled) \n",
        "  test_features = tf.convert_to_tensor(x_test_scaled) \n",
        "  train_labels = tf.convert_to_tensor(y_train) \n",
        "  test_labels = tf.convert_to_tensor(y_test)\n",
        "\n",
        "  def build_and_compile_ff(layers, neurons, dropout, decay, clipping, lr):\n",
        "    model = Sequential()\n",
        "    for layer in range(0,layers):\n",
        "      model.add(Dense(neurons))\n",
        "      model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer=tf.keras.optimizers.Adam(weight_decay=decay, clipnorm=clipping, learning_rate=lr))\n",
        "    return model\n",
        "\n",
        "  feed_forward = build_and_compile_ff(4, 50, 0.1, 0.00001, None, 0.0005)\n",
        "  history = feed_forward.fit(\n",
        "      train_features,\n",
        "      train_labels,\n",
        "      validation_split=0.2,\n",
        "      verbose=0,\n",
        "      epochs=120\n",
        "  )\n",
        "\n",
        "  y_pred = feed_forward.predict(test_features)\n",
        "  y_fit = feed_forward.predict(train_features)\n",
        "\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)"
      ],
      "metadata": {
        "id": "gc6lsU_rswVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM NN (Recurrent)"
      ],
      "metadata": {
        "id": "_FudxRDGtaB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_lstm_nn():\n",
        "  from tensorflow.python.ops.numpy_ops import np_config\n",
        "  np_config.enable_numpy_behavior()\n",
        "  model_name='lstm_nn'\n",
        "  n_hid_layers = 6\n",
        "  n_units = 180\n",
        "  n_epochs = 180\n",
        "  clipnorm = 1\n",
        "\n",
        "  #Code without windowing\n",
        "  train_features = tf.convert_to_tensor(x_train_scaled) #train_features, scaled\n",
        "  train_features = train_features.reshape((train_features.shape[0], train_features.shape[1], 1))\n",
        "  test_features = tf.convert_to_tensor(x_test_scaled) #test_features, scaled\n",
        "  test_features = test_features.reshape((test_features.shape[0], test_features.shape[1], 1))\n",
        "  train_labels = tf.convert_to_tensor(y_train) #train_labels\n",
        "  test_labels = tf.convert_to_tensor(y_test) #test_labels\n",
        "\n",
        "\n",
        "  lstm = Sequential()\n",
        "  for layer in range(0,n_hid_layers):\n",
        "    lstm.add(LSTM(n_units, return_sequences=True))\n",
        "    lstm.add(Dropout(0.05))\n",
        "  lstm.add(LSTM(n_units, return_sequences=False))\n",
        "  lstm.add(Dense(1))\n",
        "  lstm.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(clipnorm=clipnorm))\n",
        "  history = lstm.fit(\n",
        "      train_features,\n",
        "      train_labels,\n",
        "      validation_split=0.2,\n",
        "      verbose=0,\n",
        "      epochs=n_epochs,\n",
        "  )\n",
        "\n",
        "  y_pred = lstm.predict(test_features)\n",
        "  y_fit = lstm.predict(train_features)\n",
        "\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)\n",
        "  save_fits_predictions(model_predictions=y_pred, model_fits=y_fit, model_name=model_name, etf=etf)"
      ],
      "metadata": {
        "id": "fYdYOvGN5dyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_lstm_windowing_nn(n_hid_layers, n_units, n_epochs):\n",
        "  model_name='lstm_nn_w'\n",
        "  n_future = 1   # Forecast window length\n",
        "  n_past = 3  # Past window length\n",
        "  #n_past = int(0.1*len(x_train_scaled))  # Past window length\n",
        "  \n",
        "  train_features = []\n",
        "  train_labels = []\n",
        "  test_features = []\n",
        "  test_labels = []\n",
        "\n",
        "  #Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
        "  for i in range(n_past, len(x_train_scaled) - n_future +1):\n",
        "      train_features.append(x_train_scaled.iloc[i - n_past:i, 0:x_train_scaled.shape[1]])\n",
        "      train_labels.append(y_train.iloc[i + n_future - 1:i + n_future, 0])\n",
        "  train_features, train_labels = np.array(train_features), np.array(train_labels)\n",
        "\n",
        "  #Add the periods of the tarining data that are part of the required window length to the test data\n",
        "  x_test_lstm = pd.concat([x_train_scaled.iloc[len(x_train_scaled)-n_past:,:], x_test_scaled]).reset_index(drop=True)\n",
        "\n",
        "  for i in range(n_past, len(x_test_lstm) - n_future +1):\n",
        "      test_features.append(x_test_lstm.iloc[i - n_past:i, 0:x_test_lstm.shape[1]])\n",
        "      test_labels.append(y_test.iloc[i-n_past: i- n_past+1, 0])\n",
        "  test_features, test_labels = np.array(test_features), np.array(test_labels)\n",
        "\n",
        "\n",
        "  train_features = tf.convert_to_tensor(train_features) #train_features, scaled\n",
        "  test_features = tf.convert_to_tensor(test_features) #test_features, scaled\n",
        "  train_labels = tf.convert_to_tensor(train_labels) #train_labels\n",
        "  test_labels = tf.convert_to_tensor(test_labels) #test_labels\n",
        "\n",
        "  lstm = Sequential()\n",
        "  for layer in range(0,n_hid_layers):\n",
        "    lstm.add(LSTM(n_units, return_sequences=True))\n",
        "    lstm.add(Dropout(0.1))\n",
        "  lstm.add(LSTM(n_units, return_sequences=False))\n",
        "  lstm.add(Dense(1))\n",
        "  lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  history = lstm.fit(\n",
        "      train_features,\n",
        "      train_labels,\n",
        "      validation_split=0.2,\n",
        "      verbose=0,\n",
        "      epochs=n_epochs,\n",
        "  )\n",
        "\n",
        "  y_pred = lstm.predict(test_features)\n",
        "  y_fit = lstm.predict(train_features)\n",
        "\n",
        "  save_results(etf=etf, model_name=model_name, y_test=y_test, y_pred=y_pred)"
      ],
      "metadata": {
        "id": "pYpCh7IrgZwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8vpis9AxWZ0"
      },
      "source": [
        "#Run Models and Save Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7tW4debm00e"
      },
      "source": [
        "Logic: For each ETF, check if model results are already stored in the results file. If not, run the models. If yes, move on to next ETF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJcIM4ajsCZ2"
      },
      "outputs": [],
      "source": [
        "def run_model(model, etf):\n",
        "  if model==models[0]:\n",
        "    run_last_price()\n",
        "  elif model==models[1]:\n",
        "    run_ols()\n",
        "  elif model==models[2]:\n",
        "    run_ridge()\n",
        "  elif model==models[3]:\n",
        "    run_lasso()\n",
        "  elif model==models[4]:\n",
        "    run_elastic_net()\n",
        "  elif model==models[5]:\n",
        "    run_decision_tree()\n",
        "  elif model==models[6]:\n",
        "    run_random_forest()\n",
        "  elif model==models[7]:\n",
        "    run_gradient_boost()\n",
        "  elif model==models[8]:\n",
        "    run_xgboost()\n",
        "  elif model==models[9]:\n",
        "    run_sv_rbf()\n",
        "  elif model==models[10]:\n",
        "    run_lgbm()\n",
        "  elif model==models[11]:\n",
        "    run_arima()\n",
        "  elif model==models[12]:\n",
        "    run_ff_nn()\n",
        "  elif model==models[13]:\n",
        "    run_lstm_nn()\n",
        "  elif model==models[14]:\n",
        "    run_geom_brown()\n",
        "  else:\n",
        "    print('Add the model to the list of models and run_model function list')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1eH1nzmnBk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99aff835-2f32-4371-9c28-8ef783cf6e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 84)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 83 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (42) highly correlated variables:\n",
            "Completed SULOV. 41 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 41 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 41 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 21 important features:\n",
            "['outlier', 'sma_cross', 'trima_cross', 'hull_fast_cross', 'd_dema_200', 'dema_cross', 'd_sma_200', 'ema_cross', 'd_natr', 'macd_hist_cross', 'aro', 'd_open', 'hull_slow_cross', 'd_maeu', 'ppo_hist', 'diminus', 'd_adx', 'psar_hist', 'd_pvi', 'd_bold', 'cfo']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 21 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "last_price\n",
            "ols\n",
            "ridge\n",
            "lasso\n",
            "elastic_net\n",
            "decision_tree\n",
            "random_forest\n",
            "gradient_boost\n",
            "xgboost\n",
            "sv_rbf\n",
            "lgbm\n",
            "arima\n",
            "ff_nn\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "98/98 [==============================] - 0s 2ms/step\n",
            "lstm_nn\n",
            "geometric_brownian\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 83)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 82 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (39) highly correlated variables:\n",
            "Completed SULOV. 43 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 43 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 43 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 23 important features:\n",
            "['outlier', 'ema_cross', 'rsi', 'fi', 'd_vwma', 'd_wma_200', 'year', 'd_high', 'cfo', 'macd_hist_cross', 'd_pvi', 'ppo_hist', 'psar_cross', 'hull_slow_cross', 'd_wma_50', 'month_sin', 'psar_hist', 'log_d_nvi', 'd_bol_width', 'arp', 'pvo_hist', 'd_atr', 'arn']\n",
            "Total Time taken for featurewiz selection = 3 seconds\n",
            "Output contains a list of 23 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "last_price\n",
            "ols\n",
            "ridge\n",
            "lasso\n",
            "elastic_net\n",
            "decision_tree\n",
            "random_forest\n",
            "gradient_boost\n",
            "xgboost\n",
            "sv_rbf\n",
            "lgbm\n",
            "arima\n",
            "ff_nn\n",
            "25/25 [==============================] - 0s 3ms/step\n",
            "98/98 [==============================] - 0s 2ms/step\n",
            "lstm_nn\n",
            "geometric_brownian\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 84)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 83 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (37) highly correlated variables:\n",
            "Completed SULOV. 46 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 46 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 46 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 21 important features:\n",
            "['outlier', 'd_kelu', 'dema_cross', 'd_trima_200', 'psar_cross', 'last_log_return', 'd_bolu', 'day_cos', 'd_open', 'hull_fast_cross', 'dmi', 'd_hull_12', 'apo', 'd_vwma', 'd_diminus', 'ppo_hist', 'month_sin', 'mi', 'd_dema_200', 'd_bold', 'stoch_d']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 21 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "last_price\n",
            "ols\n",
            "ridge\n",
            "lasso\n",
            "elastic_net\n",
            "decision_tree\n",
            "random_forest\n",
            "gradient_boost\n",
            "xgboost\n",
            "sv_rbf\n",
            "lgbm\n",
            "arima\n",
            "ff_nn\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "98/98 [==============================] - 0s 1ms/step\n",
            "lstm_nn\n",
            "geometric_brownian\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 84)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 83 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (40) highly correlated variables:\n",
            "Completed SULOV. 43 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 43 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 43 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 22 important features:\n",
            "['outlier', 'ema_cross', 'psar_cross', 'd_hull_26', 'wma_cross', 'eom', 'd_nvi', 'd_natr', 'dpo', 'mfi', 'aro', 'd_wma_200', 'last_log_return', 'macd_hist_cross', 'ppo_hist', 'hull_fast_cross', 'd_obv', 'month_sin', 'd_hull_12', 'year', 'd_trima_50', 'd_open']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 22 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "last_price\n",
            "ols\n",
            "ridge\n",
            "lasso\n",
            "elastic_net\n",
            "decision_tree\n",
            "random_forest\n",
            "gradient_boost\n",
            "xgboost\n",
            "sv_rbf\n",
            "lgbm\n",
            "arima\n",
            "ff_nn\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "98/98 [==============================] - 0s 2ms/step\n",
            "lstm_nn\n",
            "geometric_brownian\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 82)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 81 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (37) highly correlated variables:\n",
            "Completed SULOV. 44 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 44 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 44 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 20 important features:\n",
            "['outlier', 'hull_slow_cross', 'macd_hist_cross', 'sma_cross', 'apo', 'd_ema_26', 'psar_hist', 'd_sma_200', 'pvo_hist', 'd_bolu', 'day_sin', 'd_natr', 'd_open', 'd_hull_200', 'arp', 'd_obv', 'ppo_hist', 'd_diplus', 'last_log_return', 'bol_pct']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 20 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "last_price\n",
            "ols\n",
            "ridge\n",
            "lasso\n",
            "elastic_net\n",
            "decision_tree\n",
            "random_forest\n",
            "gradient_boost\n",
            "xgboost\n",
            "sv_rbf\n",
            "lgbm\n",
            "arima\n",
            "ff_nn\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "98/98 [==============================] - 0s 2ms/step\n",
            "lstm_nn\n",
            "geometric_brownian\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 84)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 83 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (40) highly correlated variables:\n",
            "Completed SULOV. 43 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 43 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 43 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 20 important features:\n",
            "['outlier', 'macd_hist_cross', 'd_wma_50', 'aro', 'd_hull_12', 'd_wma_200', 'd_low', 'd_hull_26', 'ppo_hist', 'cfo', 'd_bol_width', 'sma_cross', 'dema_cross', 'ema_cross', 'd_pvi', 'd_natr', 'psar_cross', 'd_diplus', 'year', 'trima_cross']\n",
            "Total Time taken for featurewiz selection = 2 seconds\n",
            "Output contains a list of 20 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 2 second(s)\n",
            "last_price\n",
            "ols\n",
            "ridge\n",
            "lasso\n",
            "elastic_net\n",
            "decision_tree\n",
            "random_forest\n",
            "gradient_boost\n",
            "xgboost\n",
            "sv_rbf\n",
            "lgbm\n",
            "arima\n",
            "ff_nn\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "98/98 [==============================] - 0s 1ms/step\n",
            "lstm_nn\n",
            "geometric_brownian\n",
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "#### Single_Label Regression problem ####\n",
            "    Loaded train data. Shape = (3132, 79)\n",
            "#### Single_Label Regression problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        No variables were removed since no ID or low-information variables found in data set\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 78 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (34) highly correlated variables:\n",
            "Completed SULOV. 44 features selected\n",
            "Time taken for SULOV method = 1 seconds\n",
            "Finally 44 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 44 \n",
            "Number of booster rounds = 100\n",
            "    Completed XGBoost feature selection in 0 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 21 important features:\n",
            "['outlier', 'trima_cross', 'd_volume', 'd_low', 'hull_fast_cross', 'd_obv', 'ema_cross', 'cfo', 'd_bol_width', 'd_hull_200', 'd_psar_hist', 'd_stoch_k', 'log_d_ema_26', 'macd_hist', 'd_arp', 'd_williams_r', 'sma_cross', 'd_atr', 'd_pvi', 'log_d_abau', 'd_arn']\n",
            "Total Time taken for featurewiz selection = 3 seconds\n",
            "Output contains a list of 21 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 3 second(s)\n",
            "last_price\n",
            "ols\n",
            "ridge\n",
            "lasso\n",
            "elastic_net\n",
            "decision_tree\n",
            "random_forest\n",
            "gradient_boost\n",
            "xgboost\n",
            "sv_rbf\n",
            "lgbm\n",
            "arima\n",
            "ff_nn\n",
            "25/25 [==============================] - 0s 3ms/step\n",
            "98/98 [==============================] - 0s 1ms/step\n",
            "lstm_nn\n",
            "geometric_brownian\n"
          ]
        }
      ],
      "source": [
        "for etf in keys:\n",
        "    split_scale(etfs[etf])\n",
        "    x_train_scaled, x_test_scaled = featurewiz_selection()\n",
        "    for model in models:\n",
        "        model_saved = (etf+model == results.id).any()\n",
        "        print(model)\n",
        "        if model_saved==False:\n",
        "          run_model(model, etf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}